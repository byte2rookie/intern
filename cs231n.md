# Chapt1 课程内容大纲
## 第1部分 Lecture1-3 深度学习背景知识简单介绍
KNN 和线性分类器<br>
Softmax 和 SVM 两种损失函数<br>
优化算法（SGD等）

## 第2部分 Lecture4-9 卷积神经网络
CNN及各种层次结构（卷积、池化、全连接）<br>
反向传播及计算方法<br>
优化的训练方法（Adam、Momentum、Dropout、Batch-Normalization）<br>
训练 CNN 的注意事项（参数初始化与调优）<br>
深度学习框架（TensorFlow、Caffe、Pytorch）<br>
线性CNN结构（AlexNet、VGGNet、GoogLeNet、ResNet）<br>

## 第3部分 Lecture10-16 计算机视觉应用
RNN（语言模型，image captioning等）<br>
目标检测（R-CNN、Fast / Faster R-CNN、YOLO、SSD等）<br>
语义分割（FCN、Unet、SegNet、deeplab等）<br>
神经网络可视化与可解释性<br>
生成模型与 GAN<br>
深度强化学习<br>


# Lecture1 
图片->3通道的nxn矩阵<br>
需要适应的情况：视角变化（Viewpoint variation）、大小变化（Scale variation）、形变（Deformation）、遮挡（Occlusion）、光照条件（Illumination conditions）、背景干扰（Background clutter）、类内差异（Intra-class variation）

图像分类的方法：
### 1.硬编码
也就是先识别出图像边沿，然后给出固定的规则，来判断识别出的部位<br>
效果不好
### 2.数据驱动算法
也就是model-train流程，根据训练集和测试集不断改进Model


## 最邻近算法
这属于最简单的比较方法，一般没有实际意义
## KNN
KNN是一种经典的机器学习的神经网路
他的思路是，比较两个向量之间的L1/L2距离，按照最接近的K个向量的标签进行预测
<br>关键代码实现如下：
```
import numpy as np
class NearestNeighbor(object):
  def __init__(self):
    pass
  def train(self, X, y):
    """ X 是 NxD 维的数组，每一行都是一个样本，比如一张图片，D 是样本的数据维度；
    Y 是长度为 N 的一维数组。"""
    # 最邻近分类器只是简单的记住所有的训练数据
    self.Xtr = X
    self.ytr = y
  def predict(self, X):
    """ X 是 NxD 维的数组，每一行都是一个希望预测其标签的样本 """
    num_test = X.shape[0]
    # 确保输出的标签数据类型和输入的标签格式一致，长度是测试样本数
    Ypred = np.zeros(num_test, dtype = self.ytr.dtype)
    # 循环所有测试样本数，即测试数组的行数
    for i in range(num_test):
      # 为第 i 张测试图片找到最接近的训练图片
      # 使用 L1 距离 (差值的绝对值求和)
      '''self.Xtr - X[i,:] 利用传播机制，求测试集第 i 张图片对应的行向量和
      训练集所有图片行向量的差值，得到一个一个50000x3072的差值矩阵；
      abs(self.Xtr - X[i,:] )会将矩阵所有元素求绝对值；
      然后axis = 1 会对差值矩阵按行求和，最终得到一个长度为50000的一维
      数组，存放第 i 张图片和训练集所有50000张图片的 L1 距离。'''
      distances = np.sum(np.abs(self.Xtr - X[i,:]), axis = 1)
      min_index = np.argmin(distances) # 获取距离最小的训练集图片索引
      Ypred[i] = self.ytr[min_index] # 预测第 i 张测试集图片的标签时与其最接近的训练集图片索引
    return Ypred
```
L1距离  曼哈顿距离
L2距离  欧氏距离
而KNN就是上述代码加入一个选取超参数k的操作，来选取最有效的预测结果

## 选取超参数
### 方法一：设置验证集
从train中分出一部分作为validation set
然后在validation set之上，用多个K来进行实验
选取实验结果最好的K作为超参数
```
# 假设 Xtr_rows, Ytr, Xte_rows, Yte 还是和之前一样
# Xtr_rows 是 50,000 x 3072 的矩阵
Xval_rows = Xtr_rows[:1000, :] # 取前 1000 个训练集样本作为验证集
Yval = Ytr[:1000]
Xtr_rows = Xtr_rows[1000:, :] # 剩下的 49,000 个作为训练集
Ytr = Ytr[1000:]
# 找出在验证集表现最好的超参数 k 
validation_accuracies = []
for k in [1, 3, 5, 10, 20, 50, 100]:
  # 使用一个明确的 k 值评估验证集
  nn = NearestNeighbor()
  nn.train(Xtr_rows, Ytr)
  # 这里假设一个修正过的 NearestNeighbor 类，可以把 k 值作为参数输入
  Yval_predict = nn.predict(Xval_rows, k = k)
  acc = np.mean(Yval_predict == Yval)
  print 'accuracy: %f' % (acc,)
  # 把每个 k 值和相应的准确率保存起来
  validation_accuracies.append((k, acc))
```

### 方法二：交叉验证
就是将数据集分为n份，每次取其中一份作为验证集，然后训练多次后，取所得的平均结果作为算法效益的评估
但是这个方法不常用于DL中，所需计算开支太大了。

KNN没有用到记忆参数，也就没办法进行更好的优化，这不是一种很通用的方法。
总之KNN确实易于理解，但是这种效果不是很好，现在已经很少使用了

## 线性分类器Linear classifier
这是最基础的一种记忆参数的神经网络模型
分为:score、loss 两种function
这也就是nn.linear ，原理就是y=Wx+b
目的是通过计算得到W,b两个参数的值，predict的时候就可以抛弃trainset的数据,而只使用训练好的w,b两个参数即可
### linear层的解释
可以看作是在高维向量空间中的一种分界线函数
输入的x张量在分类函数的界内和界外，上界就是postive的结果,下界则是negtive的结果
### 参数合并
将W和b合并，在input多加一维1，这样f(xi)=Wx^ 可以将Wb作为整体一起训练

线性分类由于其拟合能力存在局限，存在有无法拟合的情况发生，也就是存在有线性表现形式会产生冲突的情况时，这种分类就失灵了
比如L2距离表示的集合内部和集合外部

---
# Lecture2
Loss&Opitimizer
也就是讲解一些loss函数和优化器的
## SVM Loss
loss=(j!=yi)∑max(0,sj-syi+Δ)
总的Loss就是所有yi的loss的平均数
我的理解是，这种max(0,-)的hinge损失就是反应的样本区分的程度


### 正则化
L1正则  累加所有参数绝对值
L2正则  平方所有参数
其本质都是对loss函数增加一些对参数本身绝对值大小的限制
L2loss可以更好的得到所有维度都均匀利用的参数
L1loss则是可以更好的提取出有最有影响的特征参数

## Softmax
softmax就是一个归一化概率,将score表示成归一化后的概率
对应的是crossentropy，交叉熵函数，将概率表示成负对数形式，然后再将进行累加平均
L=-ln(softmax())

SVM的loss和score是没有物理意义的，仅仅作为人为规定
Softmax的score则是可以看作物理意义上的概率

## 几种优化策略
1.随机搜索,直接用不同的矩阵计算loss
2.本地随即搜索，在之前的矩阵W上加入随机的扰动参数w~,W+w~获得之后的矩阵参数，再计算是否变小，loss变小就更新W
3.梯度下降Gradient Descent
对所有的参数依次轻微改变，计算对应的loss改变两，然后来计算该参数上的梯度，总是调整为梯度负方向，从而让loss变小
### 梯度下降分类
SGD,mini batch GD
mini batch GD就是每一次从总体的trainset提取一部分 minibatch，来更新梯度
而SGD则是mini batchGD的一种极端情况，每次取一个样本对参数进行更新

其他还有许多常见的optimizer,比如动量、Adam等等

---
# Lecture3 神经网络与反向传播

前向传播-forward
也就是直接计算一个function
反向传播-backward
从结尾出发，根据之前的数据，利用链式法则逐次计算出所需的梯度

反向传播部分现在已有的框架大部分都解决好了
常见的激活函数

sigmoid:        1/(1+e^-x)
relu:           max(0,x)
leaky relu:     max(0.1x,x)
Elu:不用记
神经网络部分介绍略过

---
# Lecture4 卷积神经网络
感知机与多层感知机
fc层与Conv层区别
fc层与向量所有维度全连接，而Conv层则只针对部分输入向量进行连接

## Conv卷积神经网络
卷积公式
((n+2p-f)/s+1)下取整

高维卷积=多通道输入*多通道卷积核  得到的多通道卷积结果累加起来，得到一维的卷积结果
如果要增加卷积核输出的通道数，就要单独添加卷积核的组数

CNN单层结构

两组（3x3x3）卷积核,卷积结束后的结果经过一个relu激活，之后将两组激活的结果concat到一起
至于简单的CNN网络，这里不再赘述

## 特殊Conv
1x1卷积
扩张卷积

## Conv之外的卷积操作
### 池化层Pooling
Max Pooling  就是提取最大元素
Average Pooling 平均池化，可以设置他的filter维度和stride 以调整最后池化层输出的结果
### Norm层
分为batchnorm和layernorm
这两层的运用已经逐渐减少，主要是为防止梯度爆炸而设置的，将参数进行调整的层
## 卷积层特点
参数共享(Parameter sharing)：特征检测如果适用于图片的某个区域，那么它也可能适用于图片的其他区域。即在卷积过程中，不管输入有多大，一个特征探测器(滤波器)就能对整个输入的某一特征进行探测。
合理的假设：如果一个特征在计算某个空间位置 公式的时候有用，那么它在计算另一个不同位置 公式的时候也有用。
参数共享的假设是有道理的：如果在图像某些地方探测到一个水平的边界是很重要的，那么在其他一些地方也会同样是有用的，这是因为图像结构具有平移不变性。

稀疏连接(Sparsity of connections)：在每一层中，由于滤波器的尺寸限制，输入和输出之间的连接是稀疏的，每个输出值只取决于输入在局部的一小部分值
局部连接的空间大小叫做神经元的感受野（receptive field） ，它的尺寸（其实就是滤波器的空间尺寸）是一个超参数。

## 卷积流程
一般是输入层->卷积层->池化层->输出层
池化层会导致信息的丢失，所以在池化操作前一般会多进行几轮的卷积操作提取信息
经验：几个小滤波器卷积层的组合比一个大滤波器卷积层好。
常见卷积操作的公式
```
INPUT → [[CONV → RELU]*N → POOL?]*M → [FC → RELU]*K → FC
```
主要需要学的网络->resnet网络，其中的residual-block有着深远的影响

---
# Lecture5 tricks
## 激活函数
### sigmoid函数
优点-0~1之间的激活，能够表示出充分激活和未激活的两种状态
缺点：
①Sigmoid 函数饱和时使梯度消失
②Sigmoid 函数的输出不是零中心的
③指数运算计算量较大

### tanh函数
sigmoid相似的特点，相对优点是tanh是零中心的，激活的效果会更平均

### relu函数
优点：能够快速收敛，激活效果很快
缺点：不是零中心、可能产生神经元死亡的问题

### leakyrelu
相比relu解决了神经元死亡的问题，在x<0时会有很小的值
### Elu
也就是λ(e^x-1) 和x的分段函数，比relu鲁棒性更强
### Maxout
输出的是max(w1*x+b,w2*x+b)两个线性函数的结果
有Relu的线性特点，而且没有relu的神经元死亡，不过参数量提高了一倍

## 数据预处理
### Mean Subtraction
对所有训练和测试的值减去均值，从而使数据向中心收敛，训练效率提高
### 归一化
将所有像素的值都通过x-0/sqt(δ)的方式变换为0中心的正态分布
这样可以使得数据不偏离中心
### 主成分分析（Principal Component Analysis 简称PCA）
略
### 白化变换
略

## 权重初始化
### 全零初始化
错误的方法，完全不能用
### 随机初始化
在浅层神经网络常用，深层网络可能不好用
### Xavier/He初始化（校准方差）
W = np.random.randn(fan_in, fan_out) / np.sqrt(fan_in)
使用ReLu时需要注意，这种初始化可能会导致神经元死亡加快，所以采用改进的初始化方法
W = np.random.randn(fan_in, fan_out) / np.sqrt(fan_in/2)

## BatchNorm
对每个各个channel的数据单独进行归一化
类比：RGB每个通道各自归一化
## LayerNorm
对每个batch的数据进行归一化
类比：batch里所有的彩色照片整体按照所有彩色照片进行归一化
## instanceNorm
## GroupNorm
相当于累计的InstanceNorm



# Lecture6 more tricks
## Opitimizers
SGD:随机梯度优化，这样做Loss下降的速度很快,但是最后达成收敛的速度很慢
MBGD:小批次梯度优化，SGD相当于MBGD的一个极端情况，MBGD能够做到以较小的数据更新全局参数，而且小批次能减弱参数更新的震荡程度
Momentum:动量优化器，可以解决SGD遇到鞍点时在局部最优处反复震荡的问题，更新参数时会考虑之前更新的方向
Nesterov动量：对普通动量优化器的改进，其实就是普通的动量优化器，在计算梯度时用（加入动量后的）点来计算梯度
Adagrad:对SGD的改进，目的是把learningrate调整为了动态的，而不是静态,可以提高收敛速度
  他就是就是将每一维各自的历史梯度的平方叠加起来，然后更新的时候除以该历史梯度值
  缺点：如果梯度累加的很大，学习率就会变得非常小，就会陷在局部极小值点或提前停
RMSProp:对Adagrad方法的改进，避免出现局部极小值点提前停止更新的情况出现，其实就是对梯度累计添加了一个超参数decay_rate 这样可以更好控制Adagrad优化器
Adam:最终优化成果，最常见的优化器之一，beta1,2是控制动量优化+动态学习率的两个超参数

Adam简单实现:
```
eps = 1e-8
first_moment = 0  # 第一动量，用于累积梯度，加速训练
second_moment = 0  # 第二动量，用于累积梯度平方，自动调整学习率
while True:
    dW = compute_gradient(W)
    first_moment = beta1 * first_moment + (1 - beta1) * dW  # Momentum
    second_moment = beta2 * second_moment + (1 - beta2) * dW * dW  # AdaGrad / RMSProp
    W -= learning_rate * first_moment / (np.sqrt(second_moment) + eps)
```
实际情况的实现（对second_moment会有一定的处理，防止过大）
```
eps = 1e-8
first_moment = 0  # 第一动量，用于累积梯度，加速训练
second_moment = 0  # 第二动量，用于累积梯度平方，自动调整学习率
for t in range(1, num_iterations+1):
    dW = compute_gradient(W)
    first_moment = beta1 * first_moment + (1 - beta1) * dW  # Momentum
    second_moment = beta2 * second_moment + (1 - beta2) * dW * dW  # AdaGrad / RMSProp
    first_unbias = first_moment / (1 - beta1 ** t)  # 加入偏置，随次数减小，防止初始值过小
    second_unbias = second_moment / (1 - beta2 ** t)
    W -= learning_rate * first_unbias / (np.sqrt(second_unbias) + eps)
```


退火算法:一般应用于SGD,Momentum等固定学习率的情况,对learning_rate随时间进行一定的处理
① 随步数衰减:每进行几个epoch就根据一些因素降低学习率。典型的值是每过 5 个周期就将学习率减少一半，或者每 20 个周期减少到之前的 10%。
② 指数衰减
③ 1/t 衰减。

一般来说都默认使用Adam优化器进行优化

## 正则化
### L1：正则化 loss函数加入一个|w|,表示对参数大小的限制，一般会让不重要的参数都逐渐向0靠拢
### L2：正则化 向目标函数中增加一个1/2λW^2  这样可以让参数的分布更均匀，而不会出现突出的情况
L1和L2可以联合使用，也就是Elastic net regularization
### Dropout
一个比较合理的解释是：

在训练过程中，随机失活可以被认为是对完整的神经网络抽样出一些子集，每次基于输入数据只更新子网络的参数。
每个二值掩模都是一个模型，有 n 个神经元的网络有 公式 2^n种掩模。Dropout 相当于数量巨大的网络模型（共享参数）在同时被训练。
测试(推理)时为了使得这些子网络的效果集成到一个网络中，所以必须关闭dropout的功能

更多的正则化方法在使用时再去了解即可
## 迁移学习
类似于finetune,预训练模型微调
## 模型集成（Model Ensembles）
同时跑多个模型，最后取平均值，这个工作不稀奇


# Lecture7 深度学习框架
tf框架略过，现在好像都是在用pytorch了
TensorFlow 与 PyTorch 的区别是 TensorFlow 需要先显式的构造一个计算图，然后重复运行；PyTorch 每次做前向传播时都要构建一个新的图，使程序看起来更加简洁。
更多用法参考pytorch官方文档即可
他的Visdom 图形化包可以注意下，绘图用的多
静态图和动态图现在已经逐渐靠拢，所以易用性更高的pytorch占了上风
二者主要的区别只是静态图一次编译就可以多次执行，而动态图每一次小改动都会导致重新执行
静态图更符合C++静态语言特性，而动态图则符合python语言的特性，根据需求选择合适的框架即可

# Lecture8 CNN系列的经典网络
AlexNet
VGG
GoogleLeNet
ResNet

比较经典的一些概念
1x1Conv  可以实现对原有输入的channels进行压缩,把整个输入的多个通道的信息压缩到一个维度，而多组1x1的Conv kernel则是相当于以多种方式提取出信息，然后输出一个维度，这个维度包含了之前所有channel的信息，实际上实现了一种信息的压缩提取
既可以升维，相当于让信息组合的方式变多，也可以降维，相当于信息组合的方式变少
LeNet提出的BottleNeck方式处理Inception块的方法,极大降低了整体运算的计算量，加快了模型训练速度
BottleNeck的思想在后续的ResNet中也有所体现
GoogleNet提出的Inception的思路是,在每个Inception块里把所有可能的Conv,pool等提取信息的操作都试一遍，输出一个整体，其中包含了不同提取信息方式的处理结果，让信息提取更全，效果更好
而ResNet利用了VGG重复模块堆叠+GoogleLeNet的BottleNeck处理维度问题+Residul残差思想,解决了GoogleLeNet等网络都未能解决的过深网络过拟合问题

NiN BottleNeck概念提出的背景，这个方法本质上引出了对信息压缩方式的探讨，让Conv的理解加深，从而为后续ResNet,GoogleLeNet的卷积提供了便利

ResNet的改进
将BN层和ReLU层放到了 Conv操作之前
我的理解是这样做使得上一层的信息可以更大程度的被Conv提取
因为BN是让信息分布更平均,ReLU提前激活则是让信息在原有层更多信息的情况下进行损失,这样一来可以减小Conv操作以后再提取导致的信息损失

## ResNeXt 其实就是ResNet套Inception的概念,增加了每一层提取信息的筛选强度
剩下的只记录了我觉得有用的网络
## Dense ResNet思路,把residual的结果逐层复用，更好的减轻层级间信息提取的弱化，和ResNeXT其实本质上都是一个意思，只不过Dense利用的是之前每一层的信息，而NeXT只是利用的上一层的信息
## Efficient Networks -SqueezeNet 这个是利用1x1升降维度的方式  压缩-解压缩  从而使模型可以轻量化

## NAS 这个遍历选出最优网络的代价过于高昂了，所以现在用的比较少